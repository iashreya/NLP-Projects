{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Semantic Similarity Among Words"},{"metadata":{},"cell_type":"markdown","source":"This file represents the semantic similarity between words which are attained by finding word embeddings for these words. Word Embeddings are like features or attributes of a word. They store information about the word in a vector space and by semantic similarity we mean that two words having similar meaning(or words which are used mostly in the same context) will be closer in the vector space compared to the words which have dissimilar meaning."},{"metadata":{},"cell_type":"markdown","source":"## Importing libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nfrom tqdm import tqdm\nfrom scipy.spatial.distance import cosine\nfrom sklearn.decomposition import PCA\n%matplotlib notebook\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm","execution_count":26,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Importing Pre-Trained Word Embeddings"},{"metadata":{},"cell_type":"markdown","source":"Here, we will be using pre-trained glove embeddings stored in the file *glove.6B.50d.txt* . Here, 50d represents the dimensions of these embedding vectors. It is a text file which contains words followed by their word vectors."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"f = open('../input/glove.6B.50d.txt')","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will create a dictionary of words and their embeddings by parsing this file. The result is a python dictionary in which words are keys and their corresponding vectors are the values."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"embedding_values = {}\nfor line in tqdm(f):\n    value = line.split(' ')\n    word = value[0]\n    coef = np.array(value[1:], dtype='float32')\n    embedding_values[word] = coef","execution_count":5,"outputs":[{"output_type":"stream","text":"400000it [00:05, 66863.00it/s]\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"Here, we will be creating two dictionaries which will help us locate words based on their indices.<br>\n>ix_to_word : It stores the index as key and the word as value;<br>\n>word_to_ix : It stores the word as key and corresponding index as its value;"},{"metadata":{"trusted":true},"cell_type":"code","source":"ix_to_word = {}\nword_to_ix = {}\n\nfor word in tqdm(embedding_values):\n    ix_to_word[len(ix_to_word)] = word\n    word_to_ix[word] = len(ix_to_word)","execution_count":29,"outputs":[{"output_type":"stream","text":"100%|██████████| 400000/400000 [00:00<00:00, 839942.23it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":27,"outputs":[{"output_type":"stream","text":"100%|██████████| 400000/400000 [00:17<00:00, 22258.38it/s]\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"Now, let's create a function which gives us 'n' most similar words to a given word and see if the results are relevant."},{"metadata":{"trusted":true},"cell_type":"code","source":"def most_similar(word, count):\n    cos = []\n    for i in tqdm(embedding_values):\n        cos.append(cosine(embedding_values[word], embedding_values[i]))\n    temp = cos.copy()\n    temp.sort()\n    for i in range(count):\n        id = cos.index(temp[i])\n        print(ix_to_word[id])","execution_count":34,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check for the word *king*.<br>\n\nNote : The function takes some time as we are going through all the words in the dictionary and then matching the similarity for the given word."},{"metadata":{"trusted":true},"cell_type":"code","source":"most_similar('king', 10)","execution_count":37,"outputs":[{"output_type":"stream","text":"100%|██████████| 400000/400000 [00:18<00:00, 21920.25it/s]\n","name":"stderr"},{"output_type":"stream","text":"king\nprince\nqueen\nii\nemperor\nson\nuncle\nkingdom\nthrone\nbrother\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"As we can see, the results are very much as we expected them to be."},{"metadata":{},"cell_type":"markdown","source":"### Finding analogies"},{"metadata":{},"cell_type":"markdown","source":"Since now we know the similarity between two words, it could also help us find answer some analogy based questions such as <br> *“man is to king as woman is to ..?”*<br>\nThis was explained in this paper. <a href = \"https://arxiv.org/pdf/1901.09813.pdf\">Link to the paper</a>"},{"metadata":{},"cell_type":"markdown","source":"The above problem can be solved by finding the distance between first two words and based on that distance and the third word we will try to locate our answer in the vector space. Let's try to implement it."},{"metadata":{"trusted":true},"cell_type":"code","source":"def analogy(word1, word2, word3):\n    embeds = embedding_values[word2]+embedding_values[word3]-embedding_values[word1]\n\n    cos = []\n    for i in tqdm(embedding_values):\n        cos.append(cosine(embeds, embedding_values[i]))\n\n    idx = np.array(cos).argsort()[1]\n    word4 = ix_to_word[idx]\n    \n    return word4\n","execution_count":72,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"analogy('man', 'king', 'woman')","execution_count":76,"outputs":[{"output_type":"stream","text":"100%|██████████| 400000/400000 [00:17<00:00, 23124.75it/s]\n","name":"stderr"},{"output_type":"execute_result","execution_count":76,"data":{"text/plain":"'queen'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"analogy('india', 'delhi', 'italy')","execution_count":78,"outputs":[{"output_type":"stream","text":"100%|██████████| 400000/400000 [00:17<00:00, 23291.04it/s]\n","name":"stderr"},{"output_type":"execute_result","execution_count":78,"data":{"text/plain":"'rome'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}